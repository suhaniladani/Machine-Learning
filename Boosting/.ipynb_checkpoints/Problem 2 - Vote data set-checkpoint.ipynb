{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"vote/vote.data\", delimiter='\\t', header=None) \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_outputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_dataset(dataset, k):\n",
    "\n",
    "    np.random.shuffle(dataset)\n",
    "    dataset = np.array_split(dataset, k)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_outputs(dataset):\n",
    "\n",
    "    for instance in dataset:\n",
    "        if instance[-1] == \"d\":\n",
    "            instance[-1] = 1\n",
    "        else:\n",
    "            instance[-1] = -1\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def separate_attributes(dataset):\n",
    "    \"\"\"Separate inputs from outputs and return a dictionary.\n",
    "    \n",
    "    Now it is possible to convert outputs to true integers. \n",
    "    \n",
    "    Arguments:\n",
    "        dataset -- Dataset represented as numpy array of arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = {\n",
    "        \"input\": dataset[:, 0:-1],\n",
    "        \"output\": dataset[:, -1].astype(int)\n",
    "    }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \"\"\"Implementation of AdaBoost boosting method.\n",
    "    \n",
    "    AdaBoost combines weak learners to create a strong learning hypothesis.\n",
    "    Our weak learners are essentially 1-level decision trees, commonly known as \n",
    "    decision stumps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_set, testing_set):\n",
    "\n",
    "        self.training_set = training_set\n",
    "        self.testing_set = testing_set\n",
    "         # Number of training instances\n",
    "        self.m_tr = training_set[\"input\"].shape[0] \n",
    "        # Number of input attributes(The same for testing)\n",
    "        self.n_tr = training_set[\"input\"].shape[1]  \n",
    "        # Number of testing instances\n",
    "        self.m_ts = testing_set[\"input\"].shape[0]\n",
    "        # Weights for training instances\n",
    "        self.weights = np.divide(np.ones(self.m_tr), self.m_tr)  \n",
    "        # Collection of chosen weak learners\n",
    "        self.ensemble = [] \n",
    "        # Weight assigned to each weak learner\n",
    "        self.alpha = []  \n",
    "\n",
    "\n",
    "    def evaluate_stump(self, stump):\n",
    "        \"\"\"Returns the stump error in current weighted training set.\n",
    "        \n",
    "        Arguments:\n",
    "            stump -- Stump to be evaluated.\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = np.zeros(self.m_tr)  # Hypothesis for each instance\n",
    "        pred_errors = np.ones(self.m_tr)  # 0 if correct, 1 if incorrect\n",
    "        a = stump[\"attribute\"]  # Attribute index in training set\n",
    "        # Loop through instances\n",
    "        for i in range(self.m_tr):\n",
    "            value = self.training_set[\"input\"][i][a]\n",
    "            output = self.training_set[\"output\"][i]\n",
    "            if value == stump[\"value\"]:\n",
    "                predictions[i] = stump[\"state\"]\n",
    "            else:\n",
    "                predictions[i] = stump[\"state\"] * -1\n",
    "            if predictions[i] == output:\n",
    "                pred_errors[i] = 0\n",
    "\n",
    "        # Should divide by the sum of the weights, but it is always 1\n",
    "        error = np.sum(np.multiply(self.weights, pred_errors))\n",
    "\n",
    "        return error, predictions\n",
    "\n",
    "\n",
    "    def find_best_stump(self):\n",
    "        \"\"\"Return the best decision stump for current weights.\n",
    "\n",
    "        Creates 54 different decision stumps(9 attributes * 3 possible values\n",
    "        * 2 states[True or False] combinations). \n",
    "        \"\"\"\n",
    "\n",
    "        best_stump = {}\n",
    "        lowest_error = float(\"inf\")\n",
    "        possible_values = [\"y\", \"n\", \"?\"]\n",
    "        possible_states = [1, -1]\n",
    "        # Loop through attributes\n",
    "        for a in range(self.n_tr):\n",
    "            for value in possible_values:\n",
    "                for state in possible_states:\n",
    "                    # Instantiates stump\n",
    "                    stump = {\"attribute\": a}\n",
    "                    stump[\"value\"] = value\n",
    "                    # Predict this for value or -1 * this for not value\n",
    "                    stump[\"state\"] = state  \n",
    "\n",
    "                    # Calculate error for stump\n",
    "                    error, predictions = self.evaluate_stump(stump)\n",
    "                    stump[\"error\"] = error\n",
    "                    stump[\"predictions\"] = predictions\n",
    "\n",
    "                    if error < lowest_error:\n",
    "                        lowest_error = error\n",
    "                        best_stump = stump\n",
    "\n",
    "        return best_stump\n",
    "\n",
    "\n",
    "    def calculate_alpha(self, model):\n",
    "        \"\"\"Calculates alpha for the error of the given(best) model.\n",
    "        \n",
    "        Attributes:\n",
    "            model = Best predicting weak learner for time t.\n",
    "        \"\"\"\n",
    "        error = model[\"error\"]\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "        \n",
    "        return alpha\n",
    "\n",
    "\n",
    "    def update_weights(self, model, alpha):\n",
    "        \"\"\"Update weights for time t according to AdaBoost's formula.\n",
    "        -----------------------------------------------------------------------\n",
    "        for i in range(self.m_tr):\n",
    "            if model[\"predictions\"][i] != self.training_set[\"output\"][i]:\n",
    "                self.weights[i] = np.divide(self.weights[i], \n",
    "                                            2 * model[\"error\"])\n",
    "            else:\n",
    "                self.weights[i] = np.divide(self.weights[i], \n",
    "                                            2 * (1 - model[\"error\"]))\n",
    "        \"\"\" \n",
    "\n",
    "        self.weights = np.multiply(self.weights, \n",
    "                                   np.exp(-1 * alpha \n",
    "                                     * np.multiply(self.training_set[\"output\"],\n",
    "                                                   model[\"predictions\"])\n",
    "                                   )\n",
    "                       )\n",
    "        \n",
    "        self.weights = np.divide(self.weights, np.sum(self.weights))\n",
    "\n",
    "\n",
    "    def evaluate_ensemble(self):\n",
    "        #Evaluate current strong learner with the testing set\n",
    "\n",
    "        correct = 0\n",
    "        # Loop through instances\n",
    "        for i in range(self.m_ts):\n",
    "            H = 0  \n",
    "            for model in range(len(self.ensemble)):\n",
    "                # Get the attribute that the model is related with\n",
    "                a = self.ensemble[model][\"attribute\"]\n",
    "                # Get the value(class) it presents in this instance\n",
    "                value = self.testing_set[\"input\"][i][a]\n",
    "                # Predict according to model rules\n",
    "                if value == self.ensemble[model][\"value\"]:\n",
    "                    prediction = self.ensemble[model][\"state\"]\n",
    "                else:\n",
    "                    prediction = self.ensemble[model][\"state\"] * -1\n",
    "                H += self.alpha[model] * prediction\n",
    "            H = np.sign(H)  # Strong model hypothesis\n",
    "\n",
    "            if H == self.testing_set[\"output\"][i]:\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = (correct / self.m_ts) * 100  # Simple accuracy measure\n",
    "        error = 100 - accuracy\n",
    "\n",
    "        return accuracy, error\n",
    "\n",
    "\n",
    "    def boost(self, num_iterations):\n",
    "\n",
    "        accuracies = []  # Accuracy per iteration\n",
    "        errors = []  # Error per iteration\n",
    "        model_errors = []  # Errors for the best model in each iteration\n",
    "        for i in range(num_iterations):\n",
    "            best_model = self.find_best_stump()\n",
    "            model_errors.append(best_model[\"error\"] * 100)\n",
    "            self.ensemble.append(best_model)\n",
    "            self.alpha.append(self.calculate_alpha(best_model)) \n",
    "\n",
    "            results = self.evaluate_ensemble()\n",
    "            accuracies.append(results[0])\n",
    "            errors.append(results[1])\n",
    "\n",
    "            self.update_weights(best_model, self.alpha[i])\n",
    "        return accuracies, errors, model_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies = []\n",
    "cv_errors = []\n",
    "cv_model_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Round 2\n",
      "Round 3\n",
      "Round 4\n",
      "Round 5\n",
      "Round 6\n",
      "Round 7\n",
      "Round 8\n",
      "Round 9\n",
      "Round 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = 10  # Number of folds\n",
    "\n",
    "# Dataset retrieving and formatting\n",
    "dataset = pd.read_csv(\"vote/vote.data\", delimiter='\\t', header=None) \n",
    "dataset = np.array(dataset)\n",
    "dataset = format_outputs(dataset)\n",
    "dataset = fold_dataset(dataset, k)\n",
    "\n",
    "\n",
    "# Execute k-fold cross-validation\n",
    "for i in range(k):\n",
    "    print(\"Round\", i + 1)\n",
    "    testing_set = separate_attributes(dataset[i])\n",
    "    remaining_folds = np.concatenate(np.delete(dataset, i))\n",
    "    training_set = separate_attributes(remaining_folds)\n",
    "\n",
    "    ada = AdaBoost(training_set, testing_set)\n",
    "    results = ada.boost(301)\n",
    "\n",
    "    cv_accuracies.append(results[0])\n",
    "    cv_errors.append(results[1])\n",
    "    cv_model_errors.append(results[2])\n",
    "\n",
    "# Convert lists to numpy arrays for faster calculations\n",
    "cv_accuracies = np.asarray(cv_accuracies)\n",
    "cv_errors = np.asarray(cv_errors)\n",
    "cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "# Calculate the mean of the accuracies and the errors\n",
    "cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 301 is out of bounds for axis 0 with size 301",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e6ade6c6b172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m301\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 301 is out of bounds for axis 0 with size 301"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
