{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"vote/vote.data\", delimiter='\\t', header=None) \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_outputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_dataset(dataset, k):\n",
    "\n",
    "    np.random.shuffle(dataset)\n",
    "    dataset = np.array_split(dataset, k)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_outputs(dataset):\n",
    "\n",
    "    for instance in dataset:\n",
    "        if instance[-1] == \"d\":\n",
    "            instance[-1] = 1\n",
    "        else:\n",
    "            instance[-1] = -1\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def separate_attributes(dataset):\n",
    "#     Separate inputs from outputs and return a dictionary.\n",
    "    \n",
    "#     Now it is possible to convert outputs to true integers. \n",
    "    \n",
    "#     Arguments: dataset -- Dataset represented as numpy array of arrays.\n",
    "\n",
    "\n",
    "    dataset = {\n",
    "        \"input\": dataset[:, 0:-1],\n",
    "        \"output\": dataset[:, -1].astype(int)\n",
    "    }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "#     Implementation of AdaBoost boosting method.\n",
    "    \n",
    "#     AdaBoost combines weak learners to create a strong learning hypothesis.\n",
    "#     Our weak learners are essentially 1-level decision trees, commonly known as \n",
    "#     decision stumps.\n",
    "#     \n",
    "\n",
    "    def __init__(self, training_set, testing_set):\n",
    "\n",
    "        self.training_set = training_set\n",
    "        self.testing_set = testing_set\n",
    "         # Number of training instances\n",
    "        self.m_tr = training_set[\"input\"].shape[0] \n",
    "        # Number of input attributes(The same for testing)\n",
    "        self.n_tr = training_set[\"input\"].shape[1]  \n",
    "        # Number of testing instances\n",
    "        self.m_ts = testing_set[\"input\"].shape[0]\n",
    "        # Weights for training instances\n",
    "        self.weights = np.divide(np.ones(self.m_tr), self.m_tr)  \n",
    "        # Collection of chosen weak learners\n",
    "        self.ensemble = [] \n",
    "        # Weight assigned to each weak learner\n",
    "        self.alpha = []  \n",
    "\n",
    "\n",
    "    def evaluate_stump(self, stump):\n",
    "#         Returns the stump error in current weighted training set.\n",
    "        \n",
    "#         stump -- Stump to be evaluated.\n",
    "#         \n",
    "\n",
    "        predictions = np.zeros(self.m_tr)  # Hypothesis for each instance\n",
    "        pred_errors = np.ones(self.m_tr)  # 0 if correct, 1 if incorrect\n",
    "        a = stump[\"attribute\"]  # Attribute index in training set\n",
    "        # Loop through instances\n",
    "        for i in range(self.m_tr):\n",
    "            value = self.training_set[\"input\"][i][a]\n",
    "            output = self.training_set[\"output\"][i]\n",
    "            if value == stump[\"value\"]:\n",
    "                predictions[i] = stump[\"state\"]\n",
    "            else:\n",
    "                predictions[i] = stump[\"state\"] * -1\n",
    "            if predictions[i] == output:\n",
    "                pred_errors[i] = 0\n",
    "\n",
    "        # Should divide by the sum of the weights, but it is always 1\n",
    "        error = np.sum(np.multiply(self.weights, pred_errors))\n",
    "\n",
    "        return error, predictions\n",
    "\n",
    "\n",
    "    def find_best_stump(self):\n",
    "#         Return the best decision stump for current weights.\n",
    "\n",
    "#         Creates different decision stumps( attributes * 3 possible values\n",
    "#         * 2 states[True or False] combinations). \n",
    "\n",
    "\n",
    "        best_stump = {}\n",
    "        lowest_error = float(\"inf\")\n",
    "        possible_values = [\"y\", \"n\", \"?\"]\n",
    "        possible_states = [1, -1]\n",
    "        # Loop through attributes\n",
    "        for a in range(self.n_tr):\n",
    "            for value in possible_values:\n",
    "                for state in possible_states:\n",
    "                    # Instantiates stump\n",
    "                    stump = {\"attribute\": a}\n",
    "                    stump[\"value\"] = value\n",
    "                    # Predict this for value or -1 * this for not value\n",
    "                    stump[\"state\"] = state  \n",
    "\n",
    "                    # Calculate error for stump\n",
    "                    error, predictions = self.evaluate_stump(stump)\n",
    "                    stump[\"error\"] = error\n",
    "                    stump[\"predictions\"] = predictions\n",
    "\n",
    "                    if error < lowest_error:\n",
    "                        lowest_error = error\n",
    "                        best_stump = stump\n",
    "\n",
    "        return best_stump\n",
    "\n",
    "\n",
    "    def calculate_alpha(self, model):\n",
    "#         Calculates alpha for the error of the given(best) model.\n",
    "        \n",
    "#         Attributes: model = Best predicting weak learner for time t.\n",
    "#       \n",
    "        error = model[\"error\"]\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "        \n",
    "        return alpha\n",
    "\n",
    "\n",
    "    def update_weights(self, model, alpha):\n",
    "#         Update weights for time t according to AdaBoost's formula.\n",
    "\n",
    "        self.weights = np.multiply(self.weights, \n",
    "                                   np.exp(-1 * alpha \n",
    "                                     * np.multiply(self.training_set[\"output\"],\n",
    "                                                   model[\"predictions\"])\n",
    "                                   )\n",
    "                       )\n",
    "        \n",
    "        self.weights = np.divide(self.weights, np.sum(self.weights))\n",
    "\n",
    "\n",
    "    def evaluate_ensemble(self):\n",
    "        #Evaluate current strong learner with the testing set\n",
    "\n",
    "        correct = 0\n",
    "        # Loop through instances\n",
    "        for i in range(self.m_ts):\n",
    "            H = 0  \n",
    "            for model in range(len(self.ensemble)):\n",
    "                # Get the attribute that the model is related with\n",
    "                a = self.ensemble[model][\"attribute\"]\n",
    "                # Get the value(class) it presents in this instance\n",
    "                value = self.testing_set[\"input\"][i][a]\n",
    "                # Predict according to model rules\n",
    "                if value == self.ensemble[model][\"value\"]:\n",
    "                    prediction = self.ensemble[model][\"state\"]\n",
    "                else:\n",
    "                    prediction = self.ensemble[model][\"state\"] * -1\n",
    "                H += self.alpha[model] * prediction\n",
    "            H = np.sign(H)  # Strong model hypothesis\n",
    "\n",
    "            if H == self.testing_set[\"output\"][i]:\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = (correct / self.m_ts) * 100  # Simple accuracy measure\n",
    "        error = 100 - accuracy\n",
    "\n",
    "        return accuracy, error\n",
    "\n",
    "\n",
    "    def boost(self, num_iterations):\n",
    "\n",
    "        accuracies = []  # Accuracy per iteration\n",
    "        errors = []  # Error per iteration\n",
    "        model_errors = []  # Errors for the best model in each iteration\n",
    "        for i in range(num_iterations):\n",
    "            best_model = self.find_best_stump()\n",
    "            model_errors.append(best_model[\"error\"] * 100)\n",
    "            self.ensemble.append(best_model)\n",
    "            self.alpha.append(self.calculate_alpha(best_model)) \n",
    "\n",
    "            results = self.evaluate_ensemble()\n",
    "            accuracies.append(results[0])\n",
    "            errors.append(results[1])\n",
    "\n",
    "            self.update_weights(best_model, self.alpha[i])\n",
    "        return accuracies, errors, model_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies = []\n",
    "cv_errors = []\n",
    "cv_model_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = 10  # Number of folds\n",
    "\n",
    "# Dataset retrieving and formatting\n",
    "dataset = pd.read_csv(\"vote/vote.data\", delimiter='\\t', header=None) \n",
    "dataset = np.array(dataset)\n",
    "dataset = format_outputs(dataset)\n",
    "dataset = fold_dataset(dataset, k)\n",
    "\n",
    "\n",
    "# Execute k-fold cross-validation\n",
    "for i in range(k):\n",
    "    testing_set = separate_attributes(dataset[i])\n",
    "    remaining_folds = np.concatenate(np.delete(dataset, i))\n",
    "    training_set = separate_attributes(remaining_folds)\n",
    "\n",
    "#     ada = AdaBoost(training_set, testing_set)\n",
    "#     results = ada.boost(301)\n",
    "\n",
    "#     cv_accuracies.append(results[0])\n",
    "#     cv_errors.append(results[1])\n",
    "#     cv_model_errors.append(results[2])\n",
    "\n",
    "# # Convert lists to numpy arrays for faster calculations\n",
    "# cv_accuracies = np.asarray(cv_accuracies)\n",
    "# cv_errors = np.asarray(cv_errors)\n",
    "# cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "# # Calculate the mean of the accuracies and the errors\n",
    "# cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "# cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "# cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cv_accuracies[300])\n",
    "# print(cv_errors[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': array([['n', 'n', 'n', ..., 'y', 'n', 'y'],\n",
       "        ['y', 'n', 'y', ..., 'n', 'y', '?'],\n",
       "        ['y', 'y', 'y', ..., 'n', 'y', 'y'],\n",
       "        ...,\n",
       "        ['y', 'y', 'y', ..., 'n', 'y', '?'],\n",
       "        ['n', 'y', 'n', ..., 'y', 'n', 'n'],\n",
       "        ['y', 'y', 'y', ..., 'y', 'n', 'y']], dtype=object),\n",
       " 'output': array([-1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1,  1,\n",
       "        -1,  1, -1, -1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1, -1,  1,\n",
       "         1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n",
       "         1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,\n",
       "         1,  1,  1, -1, -1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1,  1,\n",
       "        -1,  1, -1,  1,  1, -1,  1, -1, -1,  1,  1,  1,  1, -1, -1, -1, -1,\n",
       "         1,  1,  1, -1, -1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n",
       "         1, -1,  1,  1,  1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,\n",
       "        -1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "         1,  1,  1, -1, -1, -1,  1,  1,  1,  1, -1, -1,  1, -1,  1,  1, -1,\n",
       "         1,  1, -1, -1, -1, -1,  1, -1,  1, -1, -1,  1,  1,  1,  1, -1, -1,\n",
       "         1, -1,  1, -1, -1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,\n",
       "         1,  1, -1, -1, -1, -1,  1,  1,  1,  1, -1, -1, -1,  1,  1, -1,  1,\n",
       "         1, -1,  1, -1,  1, -1,  1, -1, -1, -1,  1, -1,  1,  1, -1,  1,  1,\n",
       "         1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n",
       "         1, -1,  1,  1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1,  1, -1,\n",
       "        -1,  1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1,  1,  1,  1, -1,  1,\n",
       "         1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1,  1, -1,  1,\n",
       "        -1,  1, -1, -1,  1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1, -1,\n",
       "        -1, -1,  1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1,  1,  1,\n",
       "        -1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1, -1, -1,\n",
       "         1,  1,  1, -1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1, -1,\n",
       "         1,  1,  1, -1,  1, -1, -1,  1,  1, -1, -1,  1, -1, -1, -1,  1, -1,\n",
       "        -1])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_input_1 = training_set['input'][:19] #5%\n",
    "training_set_output_1 = training_set['output'][:19]\n",
    "training_set1 = {'input': training_set_input_1, 'output': training_set_output_1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_input_2 = training_set['input'][:58] #15%\n",
    "training_set_output_2 = training_set['output'][:58]\n",
    "training_set2 = {'input': training_set_input_2, 'output': training_set_output_2}\n",
    "\n",
    "training_set_input_3 = training_set['input'][:117] #30%\n",
    "training_set_output_3 = training_set['output'][:117]\n",
    "training_set3 = {'input': training_set_input_3, 'output': training_set_output_3}\n",
    "\n",
    "training_set_input_4 = training_set['input'][:196] #50%\n",
    "training_set_output_4 = training_set['output'][:196]\n",
    "training_set4 = {'input': training_set_input_4, 'output': training_set_output_4}\n",
    "\n",
    "training_set_input_5 = training_set['input'][:274] #70%\n",
    "training_set_output_5 = training_set['output'][:274]\n",
    "training_set5 = {'input': training_set_input_5, 'output': training_set_output_5}\n",
    "\n",
    "training_set_input_6 = training_set['input'][:293] #100%\n",
    "training_set_output_6 = training_set['output'][:293]\n",
    "training_set6 = {'input': training_set_input_6, 'output': training_set_output_6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada1 = AdaBoost(training_set1, testing_set)\n",
    "results = ada1.boost(301)\n",
    "\n",
    "cv_accuracies.append(results[0])\n",
    "cv_errors.append(results[1])\n",
    "cv_model_errors.append(results[2])\n",
    "\n",
    "## Convert lists to numpy arrays for faster calculations\n",
    "# cv_accuracies = np.asarray(cv_accuracies)\n",
    "# cv_errors = np.asarray(cv_errors)\n",
    "# cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "## Calculate the mean of the accuracies and the errors\n",
    "# cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "# cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "# cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.02325581395348"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cv_accuracies[0][300])\n",
    "print(cv_errors[0][300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies = []\n",
    "cv_errors = []\n",
    "cv_model_errors = []\n",
    "ada2 = AdaBoost(training_set2, testing_set)\n",
    "\n",
    "results = ada2.boost(301)\n",
    "cv_accuracies.append(results[0])\n",
    "cv_errors.append(results[1])\n",
    "cv_model_errors.append(results[2])\n",
    "\n",
    "# # Convert lists to numpy arrays for faster calculations\n",
    "# cv_accuracies = np.asarray(cv_accuracies)\n",
    "# cv_errors = np.asarray(cv_errors)\n",
    "# cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "# Calculate the mean of the accuracies and the errors\n",
    "# cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "# cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "# cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.02325581395348\n",
      "6.976744186046517\n"
     ]
    }
   ],
   "source": [
    "print(cv_accuracies[0][300])\n",
    "print(cv_errors[0][300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies = []\n",
    "cv_errors = []\n",
    "cv_model_errors = []\n",
    "ada3 = AdaBoost(training_set3, testing_set)\n",
    "\n",
    "results = ada3.boost(301)\n",
    "cv_accuracies.append(results[0])\n",
    "cv_errors.append(results[1])\n",
    "cv_model_errors.append(results[2])\n",
    "\n",
    "# # Convert lists to numpy arrays for faster calculations\n",
    "# cv_accuracies = np.asarray(cv_accuracies)\n",
    "# cv_errors = np.asarray(cv_errors)\n",
    "# cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "# Calculate the mean of the accuracies and the errors\n",
    "# cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "# cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "# cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.37209302325581\n",
      "11.627906976744185\n"
     ]
    }
   ],
   "source": [
    "print(cv_accuracies[0][300])\n",
    "print(cv_errors[0][300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies = []\n",
    "cv_errors = []\n",
    "cv_model_errors = []\n",
    "ada4 = AdaBoost(training_set4, testing_set)\n",
    "\n",
    "results = ada4.boost(301)\n",
    "cv_accuracies.append(results[0])\n",
    "cv_errors.append(results[1])\n",
    "cv_model_errors.append(results[2])\n",
    "\n",
    "# # Convert lists to numpy arrays for faster calculations\n",
    "# cv_accuracies = np.asarray(cv_accuracies)\n",
    "# cv_errors = np.asarray(cv_errors)\n",
    "# cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "# Calculate the mean of the accuracies and the errors\n",
    "# cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "# cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "# cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.69767441860465\n",
      "9.302325581395351\n"
     ]
    }
   ],
   "source": [
    "print(cv_accuracies[0][300])\n",
    "print(cv_errors[0][300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies = []\n",
    "cv_errors = []\n",
    "cv_model_errors = []\n",
    "ada5 = AdaBoost(training_set5, testing_set)\n",
    "\n",
    "results = ada5.boost(301)\n",
    "cv_accuracies.append(results[0])\n",
    "cv_errors.append(results[1])\n",
    "cv_model_errors.append(results[2])\n",
    "\n",
    "# # Convert lists to numpy arrays for faster calculations\n",
    "# cv_accuracies = np.asarray(cv_accuracies)\n",
    "# cv_errors = np.asarray(cv_errors)\n",
    "# cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "# Calculate the mean of the accuracies and the errors\n",
    "# cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "# cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "# cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.69767441860465\n",
      "9.302325581395351\n"
     ]
    }
   ],
   "source": [
    "print(cv_accuracies[0][300])\n",
    "print(cv_errors[0][300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies = []\n",
    "cv_errors = []\n",
    "cv_model_errors = []\n",
    "ada6 = AdaBoost(training_set6, testing_set)\n",
    "\n",
    "results = ada6.boost(301)\n",
    "cv_accuracies.append(results[0])\n",
    "cv_errors.append(results[1])\n",
    "cv_model_errors.append(results[2])\n",
    "\n",
    "# # Convert lists to numpy arrays for faster calculations\n",
    "# cv_accuracies = np.asarray(cv_accuracies)\n",
    "# cv_errors = np.asarray(cv_errors)\n",
    "# cv_model_errors = np.asarray(cv_model_errors)\n",
    "\n",
    "# Calculate the mean of the accuracies and the errors\n",
    "# cv_accuracies = np.divide(np.sum(cv_accuracies, axis=0), k)\n",
    "# cv_errors = np.divide(np.sum(cv_errors, axis=0), k)\n",
    "# cv_model_errors = np.divide(np.sum(cv_model_errors, axis=0), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.02325581395348\n",
      "6.976744186046517\n"
     ]
    }
   ],
   "source": [
    "print(cv_accuracies[0][300])\n",
    "print(cv_errors[0][300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
